{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tacl playing and Todo list\n",
    "\n",
    "This started as a notebook to learn to use a piece of software called TACL (i believe stands for text analysis for corpus linguistics). I wasn't able to get it fully working yet because the user manual is a mess and requires a lot of 'figuring out' just to get it running, so that's still a to-do, but I was at least able to install it and view the documentation, so it shouldn't be too hard from here on out.\n",
    "\n",
    "That is what is in the bottom half of this file.\n",
    "\n",
    "The top half of the file is a running to-do list of various tools and tutorials to do or learn how to us in the future. Some of them I have done and crossed off, but most of them are projects I hope to do over the summer. Although there is more un-done than done, but it was good to use this opportunity to find out what was out there and what I might do in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things I have done so far\n",
    "1. Got Stanza, a natrual lanuages toolkit for many languages, working with Classical chinese;\n",
    "    - I could probably also perhaps use spacy and nltk? IDK\n",
    "2. did basic tokenizing with stanza\n",
    "    - basic, in the sense that, i split it up character by charcter which is not great\n",
    "3. Got CLTK,the Classcial lanugage toolkit, installed, but couldn't do anything with it really\n",
    "    - it doesn't seem to really have support for chinese idk but the recommended instead to use udkanbun\n",
    "    - which is a parser dedicated to classical chinese\n",
    "        - but mainly trained on more 'standard' sources so a lot of the dictionaries used in it don't work for buddhist texts\n",
    "            - but some of the people on the project were buddhist studies people wo, whats up with that?\n",
    "        - this a project based in japan so has some features that are specific to the japanese understanding/reading of the source texts\n",
    "            - makes me wonder if there isn't another analyzer made in China, or Korea\n",
    "    - perhaps some/all featuers of udkanbun are found in cltk? not sure \n",
    "4. Got udkanbun up and running\n",
    "    - did a few fun things with it\n",
    "    - got a pos tagger for it working\n",
    "\n",
    "Things I want to do ~~this week~~ as soon as possible:\n",
    "1. Get TACL up and running\n",
    "    - TACL is a projects specifically for doing analysis on the corpus of texts i am working on\n",
    "        - stands for 'text analysis for corpus linguistics'\n",
    "    - I don't remember what reminded me of this project; i heard about it several years ago but I'm not sure much development has happened on it, or whether there has been anything\n",
    "        - documentation dates from August 2019\n",
    "~~2. Get the XML thing going\n",
    "    - working on that for like two weeks now ~~\n",
    "    - got it working i guess, \n",
    " \n",
    "\n",
    "2. Programming historian stuff\n",
    " - ~~Learn to use Mallet (cf. Programming historian)~~\n",
    "    - got it basically working; not sure how useful it will be\n",
    "\n",
    "- ~~- do the tf-idf tutorial~~\n",
    "     - This is useful, but not quite what I had done before\n",
    "     - not sure this will work with chinese texts; may require tokenization\n",
    "\n",
    "- do other PH text analysis tutorials\n",
    "\n",
    "- do Beautifulsoup tutorial\n",
    "\n",
    "- do https://programminghistorian.org/en/lessons/visualizing-with-bokeh\n",
    "\n",
    "- do https://programminghistorian.org/en/lessons/text-mining-with-extracted-features\n",
    "\n",
    "\n",
    "- Do NLP book\n",
    "\n",
    "- do Spacy tutorials\n",
    "\n",
    "- do gensim tutorials\n",
    "\n",
    "- do scikit-learn tutorials for feature extraction\n",
    "\n",
    "\n",
    "- watch Paul Vierthaler videos\n",
    "\n",
    "- get Jieba working as tokenizer/parser\n",
    "\n",
    "- get kakasi/mecab working\n",
    "\n",
    "6. What can I do with word2vec?\n",
    "\n",
    "7. What can I do with TensorFlow?\n",
    "\n",
    "8. What can I do with the Ctext api?\n",
    "\n",
    "9. How do I use/What can I do with the CBETA api?\n",
    "\n",
    "9. Is there a ddb api? How can I use that?\n",
    "\n",
    "10. Is there a \n",
    "\n",
    "10. Use Kanripo with emacs\n",
    "\n",
    "11. also need to finish the \"python the ~~snarky~~ hard way\" projects \n",
    "    - maybe do automate the boring stuff instead?\n",
    "\n",
    "- Maybe do the stuff on https://investigate.ai/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tacl\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/19/7f4fe07fee3e3f85df92b5c0bc4a4514f6bafdbe7582a80f9562bdd0f144/tacl-4.2.0-py3-none-any.whl (71kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 2.1MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.23.0 in /Users/tomnewhall/opt/anaconda3/lib/python3.7/site-packages (from tacl) (0.25.1)\n",
      "Requirement already satisfied: Jinja2 in /Users/tomnewhall/opt/anaconda3/lib/python3.7/site-packages (from tacl) (2.10.3)\n",
      "Requirement already satisfied: lxml in /Users/tomnewhall/opt/anaconda3/lib/python3.7/site-packages (from tacl) (4.4.1)\n",
      "Collecting colorlog (from tacl)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/0d/22c73c2eccb21dd3498df7d22c0b1d4a30f5a5fb3feb64e1ce06bc247747/colorlog-4.1.0-py2.py3-none-any.whl\n",
      "Collecting biopython (from tacl)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/6c/0e0aedf84ccf09c279170f4da9bcef9e56c2812426f603fe62e1cae74596/biopython-1.76-cp37-cp37m-macosx_10_6_intel.whl (2.4MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4MB 6.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /Users/tomnewhall/opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.23.0->tacl) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/tomnewhall/opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.23.0->tacl) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/tomnewhall/opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.23.0->tacl) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/tomnewhall/opt/anaconda3/lib/python3.7/site-packages (from Jinja2->tacl) (1.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tomnewhall/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.23.0->tacl) (1.12.0)\n",
      "Installing collected packages: colorlog, biopython, tacl\n",
      "Successfully installed biopython-1.76 colorlog-4.1.0 tacl-4.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tacl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-00431afd61a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtacl\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtacl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtacl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'h' is not defined"
     ]
    }
   ],
   "source": [
    "import tacl as tacl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tacl [-h]\r\n",
      "            {align,catalogue,counts,diff,excise,highlight,intersect,lifetime,ngrams,prepare,results,sdiff,search,sintersect,stats,strip}\r\n",
      "            ...\r\n",
      "\r\n",
      "Analyse the text of corpora in various simple ways.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "\r\n",
      "subcommands:\r\n",
      "  {align,catalogue,counts,diff,excise,highlight,intersect,lifetime,ngrams,prepare,results,sdiff,search,sintersect,stats,strip}\r\n",
      "    align               Show aligned sets of matches between two witnesses\r\n",
      "                        side by side.\r\n",
      "    catalogue           Generate a catalogue file.\r\n",
      "    counts              List counts of n-grams in each labelled witness.\r\n",
      "    diff                List n-grams unique to each sub-corpus.\r\n",
      "    excise              Remove specified n-grams from specified works'\r\n",
      "                        witnesses.\r\n",
      "    highlight           Output a witness with its matches visually\r\n",
      "                        highlighted.\r\n",
      "    intersect           List n-grams common to all sub-corpora.\r\n",
      "    lifetime            Generate a report on the lifetime of n-grams.\r\n",
      "    ngrams              Generate n-grams from a corpus.\r\n",
      "    prepare             Convert CBETA TEI XML files into an XML form suitable\r\n",
      "                        for stripping.\r\n",
      "    results             Modify a query results file.\r\n",
      "    sdiff               List n-grams unique to each results file.\r\n",
      "    search              List witnesses containing at least one of the supplied\r\n",
      "                        n-grams.\r\n",
      "    sintersect          List n-grams common to all results files.\r\n",
      "    stats               Generate summary statistics for a set of results.\r\n",
      "    strip               Generate files for use with TACL from a corpus of TEI\r\n",
      "                        XML.\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tacl -h #now that is a useful trick to run a command like it was the command line; i couldn't figure out how to do this on the actual command line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align\n",
      "catalogue\n",
      "counts\n",
      "diff\n",
      "excise\n",
      "highlight\n",
      "intersect\n",
      "lifetime\n",
      "ngrams\n",
      "prepare\n",
      "results\n",
      "sdiff\n",
      "search\n",
      "sintersect\n",
      "stats\n",
      "strip\n"
     ]
    }
   ],
   "source": [
    "# this doesn't work\n",
    "listy = [\"align\",\"catalogue\",\"counts\",\"diff\",\"excise\",\"highlight\",\"intersect\",\"lifetime\",\"ngrams\",\"prepare\",\"results\",\"sdiff\",\"search\",\"sintersect\",\"stats\",\"strip\"]\n",
    "for x in listy:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tacl align [-h] [-v] [-m MINIMUM] [-t {cbeta,latin,pagel}]\n",
      "                  CORPUS OUTPUT RESULTS\n",
      "\n",
      "Generates an HTML report giving tables showing aligned sequences of text\n",
      "between each witness within each label and all of the witnesses in the other\n",
      "labels, within a set of results. This functionality is only appropriate for\n",
      "intersect results.\n",
      "\n",
      "positional arguments:\n",
      "  CORPUS                Path to corpus.\n",
      "  OUTPUT                Directory to output alignment files to.\n",
      "  RESULTS               Path to CSV results; use - for stdin.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -m MINIMUM, --minimum MINIMUM\n",
      "                        Minimum size of n-gram to base sequences around.\n",
      "                        (default: 20)\n",
      "  -t {cbeta,latin,pagel}, --tokenizer {cbeta,latin,pagel}\n",
      "                        Type of tokenizer to use. The \"cbeta\" tokenizer is\n",
      "                        suitable for the Chinese CBETA corpus (tokens are\n",
      "                        single characters or workaround clusters within square\n",
      "                        brackets). The \"pagel\" tokenizer is for use with the\n",
      "                        transliterated Tibetan corpus (tokens are sets of word\n",
      "                        characters plus some punctuation used to transliterate\n",
      "                        characters). (default: cbeta)\n",
      "\n",
      "Due to encoding issues, you may need to set the environment variable\n",
      "PYTHONIOENCODING to \"utf-8\".\n",
      "\n",
      "This function requires the Biopython suite of software to be installed. It is\n",
      "extremely slow and resource hungry when the overlap between two witnesses is\n",
      "very great.\n",
      "\u001b[0musage: tacl catalogue [-h] [-v] [-l LABEL] CORPUS CATALOGUE\n",
      "\n",
      "Generate a catalogue file.\n",
      "\n",
      "positional arguments:\n",
      "  CORPUS                Path to corpus.\n",
      "  CATALOGUE             Path to catalogue file.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -l LABEL, --label LABEL\n",
      "                        Label to use for all works. (default: )\n",
      "\n",
      "This command is just a convenience for generating a base catalogue file to\n",
      "then be customised manually.\n",
      "\u001b[0musage: tacl counts [-h] [-v] [-m] [-r RAM] [-t {cbeta,latin,pagel}]\n",
      "                   DATABASE CORPUS CATALOGUE\n",
      "\n",
      "List counts of n-grams in each labelled witness.\n",
      "\n",
      "positional arguments:\n",
      "  DATABASE              Path to database file.\n",
      "  CORPUS                Path to corpus.\n",
      "  CATALOGUE             Path to catalogue file.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -m, --memory          Use RAM for temporary database storage.\n",
      "                        \n",
      "                        This may cause an out of memory error, in which case\n",
      "                        run the command without this switch. (default: False)\n",
      "  -r RAM, --ram RAM     Number of gigabytes of RAM to use. (default: 3)\n",
      "  -t {cbeta,latin,pagel}, --tokenizer {cbeta,latin,pagel}\n",
      "                        Type of tokenizer to use. The \"cbeta\" tokenizer is\n",
      "                        suitable for the Chinese CBETA corpus (tokens are\n",
      "                        single characters or workaround clusters within square\n",
      "                        brackets). The \"pagel\" tokenizer is for use with the\n",
      "                        transliterated Tibetan corpus (tokens are sets of word\n",
      "                        characters plus some punctuation used to transliterate\n",
      "                        characters). (default: cbeta)\n",
      "\n",
      "Due to encoding issues, you may need to set the environment variable\n",
      "PYTHONIOENCODING to \"utf-8\".\n",
      "\u001b[0musage: tacl diff [-h] [-a LABEL] [-v] [-m] [-r RAM] [-t {cbeta,latin,pagel}]\n",
      "                 DATABASE CORPUS CATALOGUE\n",
      "\n",
      "List n-grams unique to each sub-corpus (as defined by the labels in the\n",
      "specified catalogue file).\n",
      "\n",
      "positional arguments:\n",
      "  DATABASE              Path to database file.\n",
      "  CORPUS                Path to corpus.\n",
      "  CATALOGUE             Path to catalogue file.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -a LABEL, --asymmetric LABEL\n",
      "                        Label of sub-corpus to restrict results to. (default:\n",
      "                        None)\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -m, --memory          Use RAM for temporary database storage.\n",
      "                        \n",
      "                        This may cause an out of memory error, in which case\n",
      "                        run the command without this switch. (default: False)\n",
      "  -r RAM, --ram RAM     Number of gigabytes of RAM to use. (default: 3)\n",
      "  -t {cbeta,latin,pagel}, --tokenizer {cbeta,latin,pagel}\n",
      "                        Type of tokenizer to use. The \"cbeta\" tokenizer is\n",
      "                        suitable for the Chinese CBETA corpus (tokens are\n",
      "                        single characters or workaround clusters within square\n",
      "                        brackets). The \"pagel\" tokenizer is for use with the\n",
      "                        transliterated Tibetan corpus (tokens are sets of word\n",
      "                        characters plus some punctuation used to transliterate\n",
      "                        characters). (default: cbeta)\n",
      "\n",
      "Many of the n-grams that are distinct to each sub-corpus are uninteresting -\n",
      "if a 2-gram is distinct, then so is every gram larger than 2 that contains\n",
      "that 2-gram. Therefore the results output by this command are filtered to keep\n",
      "only the most distinctive n-grams, according to the following rules (which\n",
      "apply within the context of a given witness):\n",
      "\n",
      "* If an n-gram is not composed of any (n-1)-grams found in the\n",
      "  results, it is kept.\n",
      "\n",
      "* If both of the (n-1)-grams that comprise an n-gram are found in\n",
      "  the results, that n-gram is kept.\n",
      "\n",
      "* Otherwise, the n-gram is removed from the results.\n",
      "\n",
      "examples:\n",
      "\n",
      "  Make a diff query against a CBETA corpus.\n",
      "    tacl diff cbeta2-10.db corpus/cbeta/ dhr-vs-rest.txt > output.csv\n",
      "\n",
      "  Make an asymmetrical diff query against a CBETA corpus.\n",
      "    tacl diff -a Dhr cbeta2-10.db corpus/cbeta/ dhr-vs-rest.txt > output.csv\n",
      "\n",
      "  Make a diff query against a Pagel corpus.\n",
      "    tacl diff -t pagel pagel1-7.db corpus/pagel/ by-author.txt > output.csv\n",
      "\n",
      "Due to encoding issues, you may need to set the environment variable\n",
      "PYTHONIOENCODING to \"utf-8\".\n",
      "\u001b[0musage: tacl excise [-h] [-v] [-t {cbeta,latin,pagel}]\n",
      "                   NGRAMS REPLACEMENT OUTPUT CORPUS WORK [WORK ...]\n",
      "\n",
      "Output witness files for each specified work with all of the specified n-grams\n",
      "replaced with the supplied replacement text. The replacement is done for each\n",
      "n-gram in turn, in descending order of n-gram length.\n",
      "\n",
      "positional arguments:\n",
      "  NGRAMS                Path to file containing n-grams (one per line) to be\n",
      "                        replaced.\n",
      "  REPLACEMENT           Text to replace n-grams with. This should be one or\n",
      "                        more valid tokens.\n",
      "  OUTPUT                Path to directory to output transformed files to.\n",
      "  CORPUS                Path to corpus.\n",
      "  WORK                  Work whose witnesses will be transformed.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity.\n",
      "  -t {cbeta,latin,pagel}, --tokenizer {cbeta,latin,pagel}\n",
      "                        Type of tokenizer to use. The \"cbeta\" tokenizer is\n",
      "                        suitable for the Chinese CBETA corpus (tokens are\n",
      "                        single characters or workaround clusters within square\n",
      "                        brackets). The \"pagel\" tokenizer is for use with the\n",
      "                        transliterated Tibetan corpus (tokens are sets of word\n",
      "                        characters plus some punctuation used to transliterate\n",
      "                        characters).\n",
      "\u001b[0musage: tacl highlight [-h] [-v] [-m NGRAMS] (-n NGRAMS | -r RESULTS)\n",
      "                      [-l LABEL] [-t {cbeta,latin,pagel}]\n",
      "                      CORPUS BASE_NAME OUTPUT\n",
      "\n",
      "Output an HTML report for each witness to a work, showing the text of that\n",
      "witness with supplied n-grams visually highlighted.\n",
      "\n",
      "positional arguments:\n",
      "  CORPUS                Path to corpus.\n",
      "  BASE_NAME             Name of work to display.\n",
      "  OUTPUT                Directory to output report to.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -m NGRAMS, --minus-ngrams NGRAMS\n",
      "                        Path to file containing n-grams (one per line) to\n",
      "                        remove highlighting from. This applies only when -n is\n",
      "                        used. (default: None)\n",
      "  -n NGRAMS, --ngrams NGRAMS\n",
      "                        Path to file containing n-grams (one per line) to\n",
      "                        highlight. This option may be specified multiple\n",
      "                        times; the n-grams in each file will be displayed in a\n",
      "                        distinct colour. (default: None)\n",
      "  -r RESULTS, --results RESULTS\n",
      "                        Path to CSV results; creates heatmap highlighting.\n",
      "                        (default: None)\n",
      "  -l LABEL, --label LABEL\n",
      "                        Label used to identify the n-grams from a file\n",
      "                        specified by -n/--ngrams. This option may be specified\n",
      "                        multiple times, and provided as many times as the\n",
      "                        -n/--ngrams option. (default: None)\n",
      "  -t {cbeta,latin,pagel}, --tokenizer {cbeta,latin,pagel}\n",
      "                        Type of tokenizer to use. The \"cbeta\" tokenizer is\n",
      "                        suitable for the Chinese CBETA corpus (tokens are\n",
      "                        single characters or workaround clusters within square\n",
      "                        brackets). The \"pagel\" tokenizer is for use with the\n",
      "                        transliterated Tibetan corpus (tokens are sets of word\n",
      "                        characters plus some punctuation used to transliterate\n",
      "                        characters). (default: cbeta)\n",
      "\n",
      "There are two possible outputs available, depending on whether the -n or -r\n",
      "option is specified.\n",
      "\n",
      "If n-grams are supplied via the -n/--ngrams option, the resulting HTML reports\n",
      "show the specified work's witness texts with those n-grams highlighted. Any\n",
      "n-grams that are specified via the -m/--minus-ngrams option will have had its\n",
      "constituent tokens unhighlighted. The -n/--ngrams option may be specified\n",
      "multiple times; each file's n-grams will be highlighted in a distinct colour.\n",
      "The -l/--labels option can be used with -n/--ngrams in order to provide labels\n",
      "for groups of n-grams. There must be as many instances of -l/--labels as there\n",
      "are of -n/--ngrams. The order of the labels matches the order of the n-grams\n",
      "files.\n",
      "\n",
      "If results are supplied via the -r/--results option, the resulting HTML\n",
      "reports contain an interactive heatmap of the results, allowing the user to\n",
      "select which witness' matches should be highlighted in the text. Multiple\n",
      "selections are possible, and the colour of the highlight of a token reflects\n",
      "how many witnesses have matches containing that token.\n",
      "\n",
      "examples:\n",
      "\n",
      "  tacl highlight -r intersect.csv corpus/stripped/ T0001 report_dir\n",
      "\n",
      "  tacl highlight -n author_markers.csv corpus/stripped/ T0001 report_dir\n",
      "\n",
      "  tacl highlight -n Dhr_markers.csv -n ZQ_markers.csv corpus/stripped/ -l Dharmaraksa -l \"Zhi Qian\" T0474 report_dir\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0musage: tacl intersect [-h] [-v] [-m] [-r RAM] [-t {cbeta,latin,pagel}]\n",
      "                      DATABASE CORPUS CATALOGUE\n",
      "\n",
      "List n-grams common to all sub-corpora (as defined by the labels in the\n",
      "specified catalogue file).\n",
      "\n",
      "positional arguments:\n",
      "  DATABASE              Path to database file.\n",
      "  CORPUS                Path to corpus.\n",
      "  CATALOGUE             Path to catalogue file.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -m, --memory          Use RAM for temporary database storage.\n",
      "                        \n",
      "                        This may cause an out of memory error, in which case\n",
      "                        run the command without this switch. (default: False)\n",
      "  -r RAM, --ram RAM     Number of gigabytes of RAM to use. (default: 3)\n",
      "  -t {cbeta,latin,pagel}, --tokenizer {cbeta,latin,pagel}\n",
      "                        Type of tokenizer to use. The \"cbeta\" tokenizer is\n",
      "                        suitable for the Chinese CBETA corpus (tokens are\n",
      "                        single characters or workaround clusters within square\n",
      "                        brackets). The \"pagel\" tokenizer is for use with the\n",
      "                        transliterated Tibetan corpus (tokens are sets of word\n",
      "                        characters plus some punctuation used to transliterate\n",
      "                        characters). (default: cbeta)\n",
      "\n",
      "examples:\n",
      "\n",
      "  Make an intersect query against a CBETA corpus.\n",
      "    tacl intersect cbeta2-10.db corpus/cbeta/ dhr-vs-rest.txt > output.csv\n",
      "\n",
      "  Make an intersect query against a Pagel corpus.\n",
      "    tacl intersect -t pagel pagel1-7.db corpus/pagel/ by-author.txt > output.csv\n",
      "\n",
      "Due to encoding issues, you may need to set the environment variable\n",
      "PYTHONIOENCODING to \"utf-8\".\n",
      "\u001b[0musage: tacl lifetime [-h] [-t {cbeta,latin,pagel}] [-v]\n",
      "                     CATALOGUE RESULTS LABEL OUTPUT\n",
      "\n",
      "Generate a report on the lifetime of n-grams in a results file.\n",
      "\n",
      "positional arguments:\n",
      "  CATALOGUE             Path to catalogue file.\n",
      "  RESULTS               Path to a results file to report on.\n",
      "  LABEL                 Label to mark as the focus of the report.\n",
      "  OUTPUT                Directory to output report to.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -t {cbeta,latin,pagel}, --tokenizer {cbeta,latin,pagel}\n",
      "                        Type of tokenizer to use. The \"cbeta\" tokenizer is\n",
      "                        suitable for the Chinese CBETA corpus (tokens are\n",
      "                        single characters or workaround clusters within square\n",
      "                        brackets). The \"pagel\" tokenizer is for use with the\n",
      "                        transliterated Tibetan corpus (tokens are sets of word\n",
      "                        characters plus some punctuation used to transliterate\n",
      "                        characters). (default: cbeta)\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "\n",
      "A lifetime report consists of:\n",
      "\n",
      "* an HTML table showing the disposition of each n-gram across the\n",
      "  ordered corpora (with texts and count ranges);\n",
      "\n",
      "* an HTML table showing, for each corpus, the n-grams that first\n",
      "  occurred, only occurred, and last occurred in that corpus; and\n",
      "\n",
      "* results files for each category (first occurred in, only\n",
      "  occurred in , last occurred in) for each corpus.\n",
      "\n",
      "This report may be generated from any results file, but is most usefully\n",
      "applied to the output of the lifetime script (in the tacl-extra package).\n",
      "\n",
      "The focus label is informative only, since often multiple lifetime reports\n",
      "will be generated, one per corpus, from the same master results file, but with\n",
      "specific filtering for the corpus in focus.\n",
      "\u001b[0musage: tacl ngrams [-h] [-v] [-c CATALOGUE] [-m] [-r RAM]\n",
      "                   [-t {cbeta,latin,pagel}]\n",
      "                   DATABASE CORPUS MINIMUM MAXIMUM\n",
      "\n",
      "Generate n-grams from a corpus.\n",
      "\n",
      "positional arguments:\n",
      "  DATABASE              Path to database file.\n",
      "  CORPUS                Path to corpus.\n",
      "  MINIMUM               Minimum size of n-gram to generate (integer).\n",
      "  MAXIMUM               Maximum size of n-gram to generate (integer).\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -c CATALOGUE, --catalogue CATALOGUE\n",
      "                        Path to a catalogue file used to restrict which works\n",
      "                        in the corpus are added. (default: None)\n",
      "  -m, --memory          Use RAM for temporary database storage.\n",
      "                        \n",
      "                        This may cause an out of memory error, in which case\n",
      "                        run the command without this switch. (default: False)\n",
      "  -r RAM, --ram RAM     Number of gigabytes of RAM to use. (default: 3)\n",
      "  -t {cbeta,latin,pagel}, --tokenizer {cbeta,latin,pagel}\n",
      "                        Type of tokenizer to use. The \"cbeta\" tokenizer is\n",
      "                        suitable for the Chinese CBETA corpus (tokens are\n",
      "                        single characters or workaround clusters within square\n",
      "                        brackets). The \"pagel\" tokenizer is for use with the\n",
      "                        transliterated Tibetan corpus (tokens are sets of word\n",
      "                        characters plus some punctuation used to transliterate\n",
      "                        characters). (default: cbeta)\n",
      "\n",
      "This command can be safely interrupted and subsequently rerun; witnesses that\n",
      "have already had their n-grams added will be skipped.\n",
      "\n",
      "If new witnesses need to be added after a database was generated, this command\n",
      "can be run again. However, the speed at which n-grams from these new witnesses\n",
      "are added will be much less than to a new database, due to the existing\n",
      "indices.\n",
      "\n",
      "If a witness has changed since a database was generated, this command will not\n",
      "update the database. In this case, generate a new database or manipulate the\n",
      "existing dataase directly to remove the witness and its associated n-grams.\n",
      "\n",
      "examples:\n",
      "\n",
      "  Create a database of 2 to 10-grams from a CBETA corpus.\n",
      "    tacl ngrams cbeta2-10.db corpus/cbeta/ 2 10\n",
      "\n",
      "  Create a database of 1 to 7-grams from a Pagel corpus.\n",
      "    tacl ngrams -t pagel pagel1-7.db corpus/pagel/ 1 7\n",
      "\n",
      "  Create a database of 1 to 7-grams from a subset of the CBETA corpus.\n",
      "    tacl ngrams -c dhr-texts.txt cbeta-dhr1-7.db corpus/cbeta/ 1 7\n",
      "\u001b[0musage: tacl prepare [-h] [-v] [-s SOURCE] INPUT OUTPUT\n",
      "\n",
      "Convert CBETA TEI XML files (which may have multiple files per work) into XML\n",
      "suitable for processing via the tacl strip command.\n",
      "\n",
      "positional arguments:\n",
      "  INPUT                 Directory containing XML files to prepare.\n",
      "  OUTPUT                Directory to output prepared files to.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -s SOURCE, --source SOURCE\n",
      "                        Source of TEI files. (default: cbeta-github)\n",
      "\n",
      "The TEI source options are:\n",
      "\n",
      "* cbeta-github: The CBETA TEI files as distributed on their GitHub repository\n",
      "  at https://github.com/cbeta-org/xml-p5.git.\n",
      "\u001b[0musage: tacl results [-h] [-v] [-b CORPUS] [--max-be-count COUNT] [-e CORPUS]\n",
      "                    [--excise NGRAM] [-l LABEL] [--min-count COUNT]\n",
      "                    [--max-count COUNT] [--min-count-work COUNT]\n",
      "                    [--max-count-work COUNT] [--min-size SIZE]\n",
      "                    [--max-size SIZE] [--min-works COUNT] [--max-works COUNT]\n",
      "                    [--ngrams NGRAMS] [--reciprocal] [--reduce]\n",
      "                    [--relabel CATALOGUE] [--remove LABEL] [--sort]\n",
      "                    [-t {cbeta,latin,pagel}] [-z CORPUS] [--add-label-count]\n",
      "                    [--add-label-work-count] [--collapse-witnesses]\n",
      "                    [--group-by-ngram CATALOGUE] [--group-by-witness]\n",
      "                    RESULTS\n",
      "\n",
      "Modify a query results file by adding, removing or otherwise manipulating\n",
      "result rows. Outputs the new set of results.\n",
      "\n",
      "positional arguments:\n",
      "  RESULTS               Path to CSV results; use - for stdin.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -e CORPUS, --extend CORPUS\n",
      "                        Extend the results to list the highest size grams that\n",
      "                        also count as matches, going beyond the maximum size\n",
      "                        recorded in the database. This has no effect if the\n",
      "                        results contain only 1-grams. (default: None)\n",
      "  --excise NGRAM        Remove all results whose n-gram contains the supplied\n",
      "                        n-gram within it. (default: None)\n",
      "  -l LABEL, --label LABEL\n",
      "                        Label to restrict prune requirements to (default:\n",
      "                        None)\n",
      "  --min-count COUNT     Minimum total count per n-gram to include. (default:\n",
      "                        None)\n",
      "  --max-count COUNT     Maximum total count per n-gram to include. (default:\n",
      "                        None)\n",
      "  --min-count-work COUNT\n",
      "                        Minimum count per n-gram per work to include; if a\n",
      "                        single witness meets this criterion for an n-gram, all\n",
      "                        instances of that n-gram are kept. (default: None)\n",
      "  --max-count-work COUNT\n",
      "                        Maximum count per n-gram per work to include; if a\n",
      "                        single witness meets this criterion for an n-gram, all\n",
      "                        instances of that n-gram are kept. (default: None)\n",
      "  --min-size SIZE       Minimum size of n-grams to include. (default: None)\n",
      "  --max-size SIZE       Maximum size of n-grams to include. (default: None)\n",
      "  --min-works COUNT     Minimum count of works containing n-gram to include.\n",
      "                        (default: None)\n",
      "  --max-works COUNT     Maximum count of works containing n-gram to include.\n",
      "                        (default: None)\n",
      "  --ngrams NGRAMS       Path to file containing n-grams (one per line) to\n",
      "                        exclude. (default: None)\n",
      "  --reciprocal          Remove n-grams that are not attested by at least one\n",
      "                        work in each labelled set of works. This can be useful\n",
      "                        after reducing a set of intersection results.\n",
      "                        (default: False)\n",
      "  --reduce              Remove n-grams that are contained in larger n-grams.\n",
      "                        (default: False)\n",
      "  --relabel CATALOGUE   Relabel results according to the supplied catalogue.\n",
      "                        (default: None)\n",
      "  --remove LABEL        Remove labelled results. (default: None)\n",
      "  --sort                Sort the results. (default: False)\n",
      "  -t {cbeta,latin,pagel}, --tokenizer {cbeta,latin,pagel}\n",
      "                        Type of tokenizer to use. The \"cbeta\" tokenizer is\n",
      "                        suitable for the Chinese CBETA corpus (tokens are\n",
      "                        single characters or workaround clusters within square\n",
      "                        brackets). The \"pagel\" tokenizer is for use with the\n",
      "                        transliterated Tibetan corpus (tokens are sets of word\n",
      "                        characters plus some punctuation used to transliterate\n",
      "                        characters). (default: cbeta)\n",
      "  -z CORPUS, --zero-fill CORPUS\n",
      "                        Add rows with a count of 0 for each n-gram in each\n",
      "                        witness of a work that has at least one witness\n",
      "                        bearing that n-gram. (default: None)\n",
      "\n",
      "bifurcated extend:\n",
      "  -b CORPUS, --bifurcated-extend CORPUS\n",
      "                        Extend results to bifurcation points. Generates\n",
      "                        results containing those n-grams, derived from the\n",
      "                        original n-grams, that have a label count higher than\n",
      "                        their containing (n+1)-grams, or that have a label\n",
      "                        count of one and the constituent (n-1)-grams have a\n",
      "                        higher label count. (default: None)\n",
      "  --max-be-count COUNT  Maximum size of n-gram to extend to (default: None)\n",
      "\n",
      "format changing arguments:\n",
      "  These arguments change the format of the results, making them potentially\n",
      "  unsafe to use other operations on.\n",
      "\n",
      "  --add-label-count     Output the supplied results with an additional column,\n",
      "                        \"label count\", giving the total count for each n-gram\n",
      "                        within the label. For each work, the maximum count\n",
      "                        across all of that work's witnesses is used in the\n",
      "                        sum. (default: False)\n",
      "  --add-label-work-count\n",
      "                        Output the supplied results with an additional column,\n",
      "                        \"label work count\", giving the total count of works\n",
      "                        that contain the n-gram within the label. For each\n",
      "                        work, any number of positive counts across all of that\n",
      "                        work's witnesses is counted as one in the sum.\n",
      "                        (default: False)\n",
      "  --collapse-witnesses  Collapse result rows for multiple witnesses having the\n",
      "                        same count for an n-gram. Instead of the \"siglum\"\n",
      "                        column, all of the witnesses (per work) with the same\n",
      "                        n-gram count are listed, comma separated, in the\n",
      "                        \"sigla\" column. (default: False)\n",
      "  --group-by-ngram CATALOGUE\n",
      "                        Group results by n-gram, providing summary information\n",
      "                        of the works each n-gram appears in. Results are\n",
      "                        sorted by n-gram and then order of occurrence of the\n",
      "                        label in the supplied catalogue. (default: None)\n",
      "  --group-by-witness    Group results by witness, providing summary\n",
      "                        information of which n-grams appear in each witness.\n",
      "                        (default: False)\n",
      "\n",
      "If more than one modifier is specified, they are applied in the following\n",
      "order: --extend, --bifurcated-extend, --reduce, --reciprocal, --excise,\n",
      "--zero-fill, --ngrams, --min/max-works, --min/max-size, --min/max-count,\n",
      "--min/max-count-work, --remove, --relabel, --sort. All of the options that\n",
      "modify the format are performed at the end, and only one should be specified.\n",
      "\n",
      "It is important to be careful with the use of --reduce. Coupled with --max-\n",
      "size, many results may be discarded without trace (since the reduce occurs\n",
      "first). Note too that performing \"reduce\" on a set of results more than once\n",
      "will make the results inaccurate!\n",
      "\n",
      "--extend applies before --reduce because it may generate results that are also\n",
      "amenable to reduction.\n",
      "\n",
      "--extend applies before --remove because it depends on there being at least\n",
      "two labels in the results in order to give correct results.\n",
      "\n",
      "--min-count and --max-count set the range within which the total count of each\n",
      "n-gram, across all works, must fall. For each work, its count is taken as the\n",
      "highest count among its witnesses.\n",
      "\n",
      "--min-works and --max-works count works rather than witnesses.\n",
      "\n",
      "If both --min-count-work and --max-count-work are specified, only those\n",
      "n-grams are kept that have at least one witness whose count falls within that\n",
      "range.\n",
      "\n",
      "-l/--label causes --min/max-count, --min/max-count-work, and --min/max-works\n",
      "to have their requirements apply within that labelled subset of results. All\n",
      "n-grams, both within the subset and outside it, that meet the criteria are\n",
      "kept, while all other n-grams are removed. Note that when applied to diff\n",
      "results, no n-grams outside those in the labelled subset will be kept.\n",
      "\n",
      "--relabel sets the label for each result row to the label for that row's work\n",
      "as specified in the supplied catalogue. If the work is not labelled in the\n",
      "catalogue, the label in the results is not changed.\n",
      "\n",
      "Since this command outputs a valid results file (except when using one of\n",
      "those options listed as changing the format), its output can be used as input\n",
      "for a subsequent tacl results command. To chain commands together without\n",
      "creating an intermediate file, pipe the commands together and use - instead of\n",
      "a filename, as:\n",
      "\n",
      "    tacl results --recriprocal results.csv | tacl results --reduce -\n",
      "\n",
      "examples:\n",
      "\n",
      "  Extend CBETA results and set a minimum total count.\n",
      "    tacl results -e corpus/cbeta/ --min-count 9 output.csv > mod-output.csv\n",
      "\n",
      "  Zero-fill CBETA results.\n",
      "    tacl results -z corpus/cbeta/ output.csv > mod-output.csv\n",
      "\n",
      "  Reduce Pagel results.\n",
      "    tacl results --reduce -t pagel output.csv > mod-output.csv\n",
      "\n",
      "Due to encoding issues, you may need to set the environment variable\n",
      "PYTHONIOENCODING to \"utf-8\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0musage: tacl sdiff [-h] [-v] [-t {cbeta,latin,pagel}] [-m] [-r RAM] -d DATABASE\n",
      "                  -l LABELS [LABELS ...] -s RESULTS [RESULTS ...]\n",
      "\n",
      "List n-grams unique to each set of results (as defined by the specified\n",
      "results files).\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -t {cbeta,latin,pagel}, --tokenizer {cbeta,latin,pagel}\n",
      "                        Type of tokenizer to use. The \"cbeta\" tokenizer is\n",
      "                        suitable for the Chinese CBETA corpus (tokens are\n",
      "                        single characters or workaround clusters within square\n",
      "                        brackets). The \"pagel\" tokenizer is for use with the\n",
      "                        transliterated Tibetan corpus (tokens are sets of word\n",
      "                        characters plus some punctuation used to transliterate\n",
      "                        characters). (default: cbeta)\n",
      "  -m, --memory          Use RAM for temporary database storage.\n",
      "                        \n",
      "                        This may cause an out of memory error, in which case\n",
      "                        run the command without this switch. (default: False)\n",
      "  -r RAM, --ram RAM     Number of gigabytes of RAM to use. (default: 3)\n",
      "  -d DATABASE, --db DATABASE\n",
      "                        Path to database file. (default: None)\n",
      "  -l LABELS [LABELS ...], --labels LABELS [LABELS ...]\n",
      "                        Labels to be assigned in order to the supplied\n",
      "                        results. (default: None)\n",
      "  -s RESULTS [RESULTS ...], --supplied RESULTS [RESULTS ...]\n",
      "                        Paths to results files to be used in the query.\n",
      "                        (default: None)\n",
      "\n",
      "The number of labels supplied must match the number of results files. The\n",
      "first label is assigned to all results in the first results file, the second\n",
      "label to all results in the second results file, etc. The labels specified in\n",
      "the results files are replaced with the supplied labels in the output.\n",
      "\n",
      "examples:\n",
      "\n",
      "    tacl sdiff -d cbeta2-10.db -l A B -s results1.csv results2.csv > output.csv\n",
      "\u001b[0musage: tacl search [-h] [-v] [-m] [-r RAM] [-t {cbeta,latin,pagel}]\n",
      "                   DATABASE CORPUS CATALOGUE [NGRAMS [NGRAMS ...]]\n",
      "\n",
      "Output results of searching the database for the supplied n-grams that occur\n",
      "within labelled witnesses.\n",
      "\n",
      "positional arguments:\n",
      "  DATABASE              Path to database file.\n",
      "  CORPUS                Path to corpus.\n",
      "  CATALOGUE             Path to catalogue file.\n",
      "  NGRAMS                Path to file containing list of n-grams to search for,\n",
      "                        with one n-gram per line. (default: None)\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -m, --memory          Use RAM for temporary database storage.\n",
      "                        \n",
      "                        This may cause an out of memory error, in which case\n",
      "                        run the command without this switch. (default: False)\n",
      "  -r RAM, --ram RAM     Number of gigabytes of RAM to use. (default: 3)\n",
      "  -t {cbeta,latin,pagel}, --tokenizer {cbeta,latin,pagel}\n",
      "                        Type of tokenizer to use. The \"cbeta\" tokenizer is\n",
      "                        suitable for the Chinese CBETA corpus (tokens are\n",
      "                        single characters or workaround clusters within square\n",
      "                        brackets). The \"pagel\" tokenizer is for use with the\n",
      "                        transliterated Tibetan corpus (tokens are sets of word\n",
      "                        characters plus some punctuation used to transliterate\n",
      "                        characters). (default: cbeta)\n",
      "\n",
      "If multiple paths to files containing n-grams are given, the combined set of\n",
      "n-grams from all files will be searched for.\n",
      "\n",
      "If no path is given, the results will include all n-grams found for all of the\n",
      "labelled witnesses in the catalogue.\n",
      "\n",
      "Due to encoding issues, you may need to set the environment variable\n",
      "PYTHONIOENCODING to \"utf-8\".\n",
      "\u001b[0musage: tacl sintersect [-h] [-v] [-m] [-r RAM] -d DATABASE -l LABELS\n",
      "                       [LABELS ...] -s RESULTS [RESULTS ...]\n",
      "\n",
      "List n-grams common to all sets of results (as defined by the specified\n",
      "results files).\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -m, --memory          Use RAM for temporary database storage.\n",
      "                        \n",
      "                        This may cause an out of memory error, in which case\n",
      "                        run the command without this switch. (default: False)\n",
      "  -r RAM, --ram RAM     Number of gigabytes of RAM to use. (default: 3)\n",
      "  -d DATABASE, --db DATABASE\n",
      "                        Path to database file. (default: None)\n",
      "  -l LABELS [LABELS ...], --labels LABELS [LABELS ...]\n",
      "                        Labels to be assigned in order to the supplied\n",
      "                        results. (default: None)\n",
      "  -s RESULTS [RESULTS ...], --supplied RESULTS [RESULTS ...]\n",
      "                        Paths to results files to be used in the query.\n",
      "                        (default: None)\n",
      "\n",
      "The number of labels supplied must match the number of results files. The\n",
      "first label is assigned to all results in the first results file, the second\n",
      "label to all results in the second results file, etc. The labels specified in\n",
      "the results files are replaced with the supplied labels in the output.\n",
      "\n",
      "examples:\n",
      "\n",
      "    tacl sintersect -d cbeta2-10.db -l A B -s results1.csv results2.csv > output.csv\n",
      "\u001b[0musage: tacl stats [-h] [-v] [-t {cbeta,latin,pagel}] CORPUS RESULTS\n",
      "\n",
      "Generate summary statistics for a set of results. This gives, for each\n",
      "witness, the total number of tokens and the count of matching tokens, and\n",
      "derived from these the percentage of the witness that is encompassed by the\n",
      "matches.\n",
      "\n",
      "positional arguments:\n",
      "  CORPUS                Path to corpus.\n",
      "  RESULTS               Path to CSV results.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --verbose         Display debug information; multiple -v options\n",
      "                        increase the verbosity. (default: None)\n",
      "  -t {cbeta,latin,pagel}, --tokenizer {cbeta,latin,pagel}\n",
      "                        Type of tokenizer to use. The \"cbeta\" tokenizer is\n",
      "                        suitable for the Chinese CBETA corpus (tokens are\n",
      "                        single characters or workaround clusters within square\n",
      "                        brackets). The \"pagel\" tokenizer is for use with the\n",
      "                        transliterated Tibetan corpus (tokens are sets of word\n",
      "                        characters plus some punctuation used to transliterate\n",
      "                        characters). (default: cbeta)\n",
      "\u001b[0musage: tacl strip [-h] [-v] INPUT OUTPUT\n",
      "\n",
      "Preprocess a corpus by stripping unwanted material from each file, creating a\n",
      "plain text file for each attested witness.\n",
      "\n",
      "positional arguments:\n",
      "  INPUT          Directory containing files to strip.\n",
      "  OUTPUT         Directory to output stripped files to.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help     show this help message and exit\n",
      "  -v, --verbose  Display debug information; multiple -v options increase the\n",
      "                 verbosity. (default: None)\n",
      "\n",
      "This command operates on files in an augmented TEI XML format that is quite\n",
      "close to that used in the CBETA GitHub files.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# basically there are several places that tacl is documented, but maybe the best docs are within tacl itself\n",
    "!tacl align -h\n",
    "!tacl catalogue -h\n",
    "!tacl counts -h\n",
    "!tacl diff -h\n",
    "!tacl excise -h\n",
    "!tacl highlight -h\n",
    "!tacl intersect -h\n",
    "!tacl lifetime -h\n",
    "!tacl ngrams -h\n",
    "!tacl prepare -h\n",
    "!tacl results -h\n",
    "!tacl sdiff -h\n",
    "!tacl search -h\n",
    "!tacl sintersect -h\n",
    "!tacl stats -h\n",
    "!tacl strip -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
